{
    "projects": [
        {
            "id": "cvz",
            "title": "Computer Vision",
            "description": "Computer Vision demostration through project-based work.",
            "logo": "../logos/computer_vision.svg",
            "subtitles": [
                "OpenCV Basics",
                "Image Processing",
                "Video Basics",
                "Object Detection",
                "Deep Learning",
                "Capstone Project"
            ],
            "sections": [
                "A basic introduction to image handling using OpenCV. OpenCV can interface with python but has core functionality in C++, making it fast enough to process real time image data.",
                "An introduction to image visualization and manipulation with techniques including: color mappings, blending and pasting, thresholding, blurring and smoothing, morphological operators, gradients, and histograms.",
                "Introduction on how to open and use saved video files, and pull video feed from camera hardware.",
                "Introduces and explains how to track objects in Python and OpenCV. Contained are instruction for what features the software detects and how the software can detect these features.",
                "Introduction to automated computerized image processing. All deep learning is done within Keras. All models are contained within the .h5 files.",
                "This is a finger counter program. With live video from the camera, a neural network counts the number of fingers being held up in a segment of the image."
            ] 
        },
        {
            "id": "ctr",
            "title": "Computed Tomography",
            "description": "CT scanner mathematical simulation with MATLAB.",
            "subtitles": [
                "CT Simulation - Initial Object to Scan",
                "CT Simulation - CT Machine Configuration",
                "CT Simulation - Sinogram - Single View",
                "CT Simulation - Full Sinogram",
                "CT Simulation - Full Simulation",
                "Algebraic Reconstruction Technique",
                "Metal Artifact Reduction",
                "MAR - Creation of Metal Mask",
                "MAR - Results of Removing Metalic Regions",
                "MAR - Interpolation of Missing Information"
            ],
            "sections": [
                "The goal of this project is to demonstrate how a CT Images are created. This is done in 3 subprojects: CT Simulation, The Algebraic Reconstruction Technique, and Metal Artifact Reduction. This project begins with the simulation. Since the machines are expensive, it is easier to being with a CT image and transform it into CT Data, called a sinogram, with software.",
                "As is the case with any simulation, a physical approximation of the machine is needed. This image shows the manin components of a CT machine, the x-ray source, the detectors, and the locations of the beams themselves. In this simulation there are many more beams than pictured, but the overall visual effectively shows what the machine looks like in this simulation.",
                "For any given position of the machine, this is a visualization of the values that the detector read. The higher the value, the more bone or harder material was present.",
                "The individual views can be stacked on top of each other to create a sinogram. Points in real space are represented as sinusoidal waves. Sinograms are simply a collection of sine waves.",
                "This is a full visualization of the process of collecting CT data. The .gif speaks for itself and shows the process of the simulation. THe virtual machine is rotated around the image and simulated detector data is collected.",
                "The reconstruction is handled through 2 loops. The first loop iterates through every view in the CT data. THe second loop creates iterations of these complete view calculation. Another note to be made is that this is a different dataset as the one referred to at the beginning of this page. This is a chest CT. Lastly, there are many processing techniques and entirely other reconstruction methods that are possible. Namely, Fourier Transform is the industry standard. This is where Sinogram Data is transformed into Fourier Data and then computed into image data. I may explore this in another project, but not for the time being.",
                "Metal in CT scans cause problems for how they affect the x-rays. The noise is pictured in this image.",
                "Masks are commonly used in imaging to identify areas of interest. In this case, the metal is a structure of high attenuation. So the idea is to isolate these regions, transform it into sinogram space, and then apply this mask to the sinogram and reconstruct.",
                "The results of the masked sinogram and masked reconstruction is pictured here. The metal data is gone, but so is much of the other relevant data. So more must be done.",
                "A simple interpolation algorithm is performed on the sinogram data using the mask called regionfill(). This is an internal function within MATLAB and worked very well to showcase how this method works. There was another method shown in my git repo."
            ]
        },
        {
            "id": "ecg",
            "title": "ECG Device",
            "description": "Design, Development, and Construction of a electrocardiogram device.",
            "logo": "",
            "subtitles": [
                "Final System",
                "Analog Design",
                "Analog PCB",
                "Analog Signal",
                "System Display",
                "Special Thanks:"
            ],
            "sections": [
                "The total system together. Unseen are the sensor and arduino board. There was no casing designed for this project.",
                "Analog Circuit Designed by Analog Team. The team had 2 weeks to design a board to fit all the filters, amplifiers, and signal connections together.",
                "Final analog circuit board. This is a handmade PCB from the above design.",
                "The signal received from the sensor, after analog processing, to the oscilloscope. This was used to tune the digital thresholding.",
                "Sample readout of heart, with high Pressure alarm. The device also includes a buzzer to indicate heart beat (during the QRS-complex).",
                "Paniz Izadi, Sevda Golkar, Andrea Rǎileanu, Akin Çaǧlayan. Also to the Professors and student tutors that provided insights and learning opportunities."
            ]
        },
        {
            "id": "isp",
            "title": "ISP Optimization",
            "description": "Optimization of an image signal processor using a differentiable proxy.\n\nFiles are owned by Basler AG",
            "logo": "",
            "subtitles": [
                "The Problem: Part 1",
                "The Problem: Part 2",
                "The Solution",
                "Step 1: Approximate the ISP",
                "Step 2: Optimize the ISP",
                "Data Used",
                "Step 1 Results: COCO",
                "Step 1 Results: HAM10000",
                "Parameter Optimization Results",
                "Cross Implementation Results",
                "Recommendations and Special Thanks"
            ],
            "sections": [
                "Cameras operate as a part of a very complicated system. Photons travel from an object, through an optics system, onto an image sensor. No object is the same. And both the optic systems and the image sensors are chosen based on application. Each of these components are variable to any application. We can see that this is already a complex system.",
                "Finally, the image sensor filters these photons by color, and registers them by strikes. THe more strikes, the higher the intensity. This creates a bayer pattern, as seen on the left of the image. The image signal processor (ISP) has to compensate for all the components and produce an image. Further, it must do this for many different application, so they are parameterized. The parameters are then optimized manually by a team of engineers and a golden-eye expert.",
                "ISP's are complex and are too complex and are considered black boxes. The inputs and outputs are known, but the processed are to complex to model mathematically to optimize the parameters. Therefor this research aimed to approximate an ISP in a model that was able to be optimized (differentiable). This is where the differentiable proxy comes in. A differentiable proxy would be a white box, in this case a U-Net neural network model.",
                "The research requires 2 overall steps. The first step was to approximate the ISP using the U-Net. This step builds the differentiable proxy. The Rawpy ISP was chosen as a cheap and easy way to begin this research, as it is free and open sourced. ISP parameters were randomized and passed into both the Rawpy ISP and the U-Net model. The difference between the outputs was then used to improve the model and the process was repeated several thousand times.",
                "The second step took these this developed model and Optimized the ISP parameters to produce more similar images. This was done by taking fixed ISP parameters (referred to ideal parameters), putting them into Rawpy, while taking random ISP parameters (referred to as tunable parameters) and putting them into the proxy. The difference between these would then be used to improve the tunable parameters to try to see if the tunable parameters would converge on the ideal parameters.",
                "This experiment was conducted on two different data sets. The first being the COCO data set (https://cocodataset.org/), which is a collection of hundreds of thousand of images tagged for image segmentation, object detection, and captioning. The second of which being the HAM10000 data set (https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000), consisting of 10000 images of skin lesions. In both cases, this project used a small subset of randomly selected images of each dataset. The selected images were then transformed into RAW data, that contains bayer patterns of the original images.",
                "Pictured are 4 random images from the data set. From left to right: the Rawpy ISP output, the differentiable proxy output, and the difference between these two images. If the differentiable proxy was perfect, the difference image would be completely black. Interesting insights were the errors at the lines and details as seen in images 637 and 1548. Also interesting are the inclusion of a hotspot in image 3215 not present in the rawpy image.",
                "The differentiable proxy performed much better on the lower diversity HAM10000 data set. Still notable problems were at the lines. And even more interesting, the differentiable proxy had a greater problem with inclusion of structures not present in the image, as seen in the splotches of light or dark spots in all images. Overall performance was better for this data set, but the model introduced more artifacts in the process.",
                "Finally, these are the results in the parameter tuning. Of the 12 parameters tested from Rawpy, 'No Auto Scaling' was fixed. Of the remaining parameters, only 'Four Color RGB' and 'Use Auto White Balance' converged on their ideal values. Though it is important to note that these are the only boolean (trie/false) parameters in the set.",
                "Another insight was gained through cross implementation of the differentiable proxies. THe proxy trained on COCO images was fed HAM10000 images and vise versa. If the model was learning how to de-bayer and process raw data effectively, this would not be the case.",
                "The results prompt the following recommendations:\nMore complex image signal processor\nDifferent approximations\nDifferent data sets\nChange Loss Calculations for Parameter Tuning\nFinally, A special thanks to the Research and Development team at Basler AG for giving the access to technology, hardware, and expertise to complete this research project. And Also thanks to the University of Applied Sciences of Lübeck to providing resources and education that made completing this project possible." 
            ]
        },
        {
            "id": "ldd",
            "title": "Leaf Disease Diagnosis",
            "description": "Agriculture application of deep learning in detecting plant disease.",
            "logo": "",
            "subtitles": [
                "Motivation and Objective",
                "The Data",
                "The Dataset",
                "The Models",
                "Results",
                "Further Work",
                "Special Thanks:"
            ],
            "sections": [
                "Early diagnosis of plant diseases allows preventative measures to reduce economic and production damage. Traditional methods are limited by available expertise. Plant surveillance by farmers is not always practical or cost-effective. Evaluate different databases and implement various neural network architectures for classifying plant diseases.",
                "The data was taken from this dataset on Kaggle. These are a collection of 256x256 images containing leaves of many different species of plants. The leaves are also labeled with their disease status.",
                "Pictured is the label count of all the data. There are thousands of images in the dataset.",
                "As with every AI project, data preprocessing had to be performed to normalize the image data for training the models. However, the main experiment performed was on the model architecture. There were 5 different CNN models tested:\n\n3 Convolutional Layers and 3 FC Layers\n3 Convolutional Layers and 4 FC Layers\n4 Convolutional Layers and 3 FC Layers\n3 Convolutional Layers and 3 FC Layers and dropout in the dense layers\n3 Convolutional Layers and 3 FC Layers and dropout in the convolutional layers",
                "The Results were promising, an accuracy of between 80-90% were the results for each model. The best results were for model 4, where dropout proved effective at improving model performance.",
                "This is a confusion matrix of the classification for the validation data. This tests how effective the model was at classifying each case. The results suggested an improvement. By separating the classification into the different types of plants, the performance could be improved. The model could be having to learn too many features. So one solution could be creating a series of small models for classifying each disease based on plant type. THe plant type would already be known by farmers, as crops are planted deliberately. Most of the functionality of a system like this would be to check for diseases on a known crop. Th other improvement would be to implement a more complicated model, like ResNet-18. This could recognize enough features to be effective.",
                "Coursera Course"
            ] 
        },
        {
            "id": "mls",
            "title": "Machine-Learned Stethoscope",
            "description": "Medical application of machine learning.",
            "logo": "",
            "subtitles": [
            ],
            "sections": [         
            ] 
        },
        {
            "id": "mct",
            "title": "Morse Code Translator",
            "description": "A simple dictionary-based Morse code translator.",
            "logo": "",
            "subtitles": [
            ],
            "sections": [         
            ] 
        },
        {
            "id": "nmr",
            "title": "Nuclear Magnetic Resonance",
            "description": "A digitalization project for a CW-NMR Machine.",
            "logo": "",
            "subtitles": [
            ],
            "sections": [         
            ] 
        }
    ],
    "logos": ["logo1", "logo2", "logo3", "logo4"]
}